{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Nano-GPT: RoPE vs Standard Position Embeddings Comparison\n",
        "\n",
        "This notebook compares Rotary Position Embeddings (RoPE) against standard absolute positional embeddings on two datasets:\n",
        "- **WikiText-2**: Encyclopedic text from Wikipedia\n",
        "- **BookCorpus**: Narrative text from books (sampled subset)\n",
        "\n",
        "**Objectives:**\n",
        "1. Compare RoPE vs standard position embeddings on WikiText-2\n",
        "2. Compare RoPE vs standard position embeddings on BookCorpus\n",
        "3. Analyze if RoPE effect is similar across different text styles\n",
        "4. Study convergence speed and final perplexities\n",
        "5. Compare text generation quality"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "from model import GPT, GPTConfig\n",
        "import tiktoken\n",
        "from contextlib import nullcontext\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import time\n",
        "import os\n",
        "import sys\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "np.random.seed(1337)\n",
        "\n",
        "sns.set_style('whitegrid')\n",
        "plt.rcParams['figure.figsize'] = (12, 6)\n",
        "\n",
        "print(\"Setup complete. Libraries imported.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "dtype = 'bfloat16' if torch.cuda.is_bf16_supported() else 'float16'\n",
        "\n",
        "batch_size = 12\n",
        "block_size = 256\n",
        "n_layer = 6\n",
        "n_head = 6\n",
        "n_embd = 384\n",
        "dropout = 0.1\n",
        "learning_rate = 1e-3\n",
        "max_iters = 3000\n",
        "eval_interval = 200\n",
        "eval_iters = 100\n",
        "warmup_iters = 100\n",
        "min_lr = 1e-4\n",
        "weight_decay = 0.1\n",
        "beta1 = 0.9\n",
        "beta2 = 0.99\n",
        "grad_clip = 1.0\n",
        "decay_lr = True\n",
        "rope_theta = 10000.0\n",
        "TARGET_TOKENS = 2_500_000\n",
        "\n",
        "print(f\"Device: {device}, Dtype: {dtype}\")\n",
        "print(f\"Model: {n_layer} layers, {n_head} heads, {n_embd} dim\")\n",
        "print(f\"Training: {max_iters} iters, batch_size={batch_size}, block_size={block_size}\")\n",
        "print(f\"RoPE theta: {rope_theta}\")\n",
        "print(f\"BookCorpus target: {TARGET_TOKENS:,} tokens\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def prepare_wikitext2():\n",
        "    train_path = 'data/wikitext2/train.bin'\n",
        "    val_path = 'data/wikitext2/val.bin'\n",
        "    \n",
        "    if os.path.exists(train_path) and os.path.exists(val_path):\n",
        "        print(\"WikiText-2 already prepared\")\n",
        "        return\n",
        "    \n",
        "    print(\"Preparing WikiText-2 dataset...\")\n",
        "    from datasets import load_dataset\n",
        "    \n",
        "    enc = tiktoken.get_encoding(\"gpt2\")\n",
        "    dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\")\n",
        "    \n",
        "    for split, split_name in [('train', 'train'), ('validation', 'val')]:\n",
        "        all_ids = []\n",
        "        for example in tqdm(dataset[split], desc=f\"Tokenizing {split}\"):\n",
        "            if example['text'].strip():\n",
        "                ids = enc.encode_ordinary(example['text'])\n",
        "                ids.append(enc.eot_token)\n",
        "                all_ids.extend(ids)\n",
        "        \n",
        "        arr = np.array(all_ids, dtype=np.uint16)\n",
        "        os.makedirs('data/wikitext2', exist_ok=True)\n",
        "        arr.tofile(f'data/wikitext2/{split_name}.bin')\n",
        "        print(f\"  {split_name}: {len(arr):,} tokens ({len(arr)*2/1024/1024:.2f} MB)\")\n",
        "    \n",
        "    print(\"WikiText-2 preparation complete\")\n",
        "\n",
        "\n",
        "def prepare_bookcorpus_sample(target_tokens=TARGET_TOKENS):\n",
        "    train_path = 'data/bookcorpus/train_sample.bin'\n",
        "    val_path = 'data/bookcorpus/val_sample.bin'\n",
        "    \n",
        "    if os.path.exists(train_path) and os.path.exists(val_path):\n",
        "        train_data = np.memmap(train_path, dtype=np.uint16, mode='r')\n",
        "        if len(train_data) >= target_tokens * 0.9:\n",
        "            print(f\"BookCorpus sample already prepared ({len(train_data):,} tokens)\")\n",
        "            return\n",
        "    \n",
        "    print(f\"Preparing BookCorpus sample (target ~{target_tokens:,} tokens)...\")\n",
        "    from datasets import load_dataset\n",
        "    \n",
        "    enc = tiktoken.get_encoding(\"gpt2\")\n",
        "    val_ratio = 0.1\n",
        "    \n",
        "    dataset = load_dataset(\"SamuelYang/bookcorpus\", split=\"train\", streaming=True)\n",
        "    \n",
        "    train_ids = []\n",
        "    val_ids = []\n",
        "    \n",
        "    for i, example in enumerate(tqdm(dataset, desc=\"Tokenizing BookCorpus\")):\n",
        "        if len(train_ids) >= target_tokens:\n",
        "            break\n",
        "        \n",
        "        text = example['text']\n",
        "        if text.strip():\n",
        "            ids = enc.encode_ordinary(text)\n",
        "            ids.append(enc.eot_token)\n",
        "            \n",
        "            if np.random.random() < val_ratio:\n",
        "                val_ids.extend(ids)\n",
        "            else:\n",
        "                train_ids.extend(ids)\n",
        "    \n",
        "    os.makedirs('data/bookcorpus', exist_ok=True)\n",
        "    \n",
        "    train_arr = np.array(train_ids[:target_tokens], dtype=np.uint16)\n",
        "    train_arr.tofile(train_path)\n",
        "    print(f\"  train_sample: {len(train_arr):,} tokens ({len(train_arr)*2/1024/1024:.2f} MB)\")\n",
        "    \n",
        "    val_arr = np.array(val_ids[:int(target_tokens*val_ratio)], dtype=np.uint16)\n",
        "    val_arr.tofile(val_path)\n",
        "    print(f\"  val_sample: {len(val_arr):,} tokens ({len(val_arr)*2/1024/1024:.2f} MB)\")\n",
        "    \n",
        "    print(\"BookCorpus sample preparation complete\")\n",
        "\n",
        "print(\"Dataset preparation functions defined!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Preparing datasets...\")\n",
        "prepare_wikitext2()\n",
        "prepare_bookcorpus_sample()\n",
        "\n",
        "print(\"\\nLoading datasets...\")\n",
        "\n",
        "wikitext2_train = np.memmap('data/wikitext2/train.bin', dtype=np.uint16, mode='r')\n",
        "wikitext2_val = np.memmap('data/wikitext2/val.bin', dtype=np.uint16, mode='r')\n",
        "\n",
        "bookcorpus_train = np.memmap('data/bookcorpus/train_sample.bin', dtype=np.uint16, mode='r')\n",
        "bookcorpus_val = np.memmap('data/bookcorpus/val_sample.bin', dtype=np.uint16, mode='r')\n",
        "\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(\"DATASET STATISTICS\")\n",
        "print(f\"{'='*60}\")\n",
        "\n",
        "print(f\"\\nWikiText-2 (Encyclopedic):\")\n",
        "print(f\"  Train: {len(wikitext2_train):,} tokens ({len(wikitext2_train)*2/1024/1024:.2f} MB)\")\n",
        "print(f\"  Val:   {len(wikitext2_val):,} tokens ({len(wikitext2_val)*2/1024/1024:.2f} MB)\")\n",
        "\n",
        "print(f\"\\nBookCorpus (Narrative - Sampled):\")\n",
        "print(f\"  Train: {len(bookcorpus_train):,} tokens ({len(bookcorpus_train)*2/1024/1024:.2f} MB)\")\n",
        "print(f\"  Val:   {len(bookcorpus_val):,} tokens ({len(bookcorpus_val)*2/1024/1024:.2f} MB)\")\n",
        "\n",
        "print(f\"\\nComparison:\")\n",
        "print(f\"  WikiText-2 / BookCorpus ratio: {len(wikitext2_train)/len(bookcorpus_train):.2f}x\")\n",
        "print(f\"  Difference: {abs(len(wikitext2_train) - len(bookcorpus_train)):,} tokens\")\n",
        "\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(\"Datasets loaded successfully!\")\n",
        "print(f\"{'='*60}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_model(vocab_size=50257, use_rope=False, rope_theta=10000.0):\n",
        "    config = GPTConfig(\n",
        "        vocab_size=vocab_size,\n",
        "        block_size=block_size,\n",
        "        n_layer=n_layer,\n",
        "        n_head=n_head,\n",
        "        n_embd=n_embd,\n",
        "        dropout=dropout,\n",
        "        bias=True,\n",
        "        use_rope=use_rope,\n",
        "        rope_theta=rope_theta,\n",
        "    )\n",
        "    return GPT(config)\n",
        "\n",
        "print(\"Creating models...\")\n",
        "\n",
        "model_wikitext_no_rope = create_model(use_rope=False)\n",
        "model_wikitext_rope = create_model(use_rope=True)\n",
        "model_bookcorpus_no_rope = create_model(use_rope=False)\n",
        "model_bookcorpus_rope = create_model(use_rope=True)\n",
        "\n",
        "models = {\n",
        "    'wikitext_no_rope': (model_wikitext_no_rope, wikitext2_train, wikitext2_val, 'WikiText-2 (No RoPE)', 'out-wikitext2-no-rope'),\n",
        "    'wikitext_rope': (model_wikitext_rope, wikitext2_train, wikitext2_val, 'WikiText-2 (RoPE)', 'out-wikitext2-rope'),\n",
        "    'bookcorpus_no_rope': (model_bookcorpus_no_rope, bookcorpus_train, bookcorpus_val, 'BookCorpus (No RoPE)', 'out-bookcorpus-no-rope'),\n",
        "    'bookcorpus_rope': (model_bookcorpus_rope, bookcorpus_train, bookcorpus_val, 'BookCorpus (RoPE)', 'out-bookcorpus-rope'),\n",
        "}\n",
        "\n",
        "num_params = model_wikitext_no_rope.get_num_params()\n",
        "\n",
        "print(f\"\\n  All models: {num_params/1e6:.2f}M parameters\")\n",
        "print(f\"  Architecture: {n_layer} layers, {n_head} heads, {n_embd} dimensions\")\n",
        "print(f\"\\n  Models created:\")\n",
        "for name, (_, _, _, display_name, _) in models.items():\n",
        "    print(f\"    - {display_name}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"Models created successfully!\")\n",
        "print(\"=\"*70)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_batch(data, batch_size, block_size, device):\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([torch.from_numpy((data[i:i+block_size]).astype(np.int64)) for i in ix])\n",
        "    y = torch.stack([torch.from_numpy((data[i+1:i+1+block_size]).astype(np.int64)) for i in ix])\n",
        "    return x.to(device), y.to(device)\n",
        "\n",
        "@torch.no_grad()\n",
        "def estimate_loss(model, train_data, val_data, batch_size, block_size, eval_iters, device, dtype_ctx):\n",
        "    model.eval()\n",
        "    losses = {}\n",
        "    \n",
        "    for split, data in [('train', train_data), ('val', val_data)]:\n",
        "        loss_list = []\n",
        "        for _ in range(eval_iters):\n",
        "            X, Y = get_batch(data, batch_size, block_size, device)\n",
        "            with dtype_ctx:\n",
        "                _, loss = model(X, Y)\n",
        "            loss_list.append(loss.item())\n",
        "        losses[split] = np.mean(loss_list)\n",
        "    \n",
        "    model.train()\n",
        "    return losses\n",
        "\n",
        "def get_lr(iter_num, warmup_iters, lr_decay_iters, learning_rate, min_lr):\n",
        "    if iter_num < warmup_iters:\n",
        "        return learning_rate * (iter_num + 1) / (warmup_iters + 1)\n",
        "    if iter_num > lr_decay_iters:\n",
        "        return min_lr\n",
        "    decay_ratio = (iter_num - warmup_iters) / (lr_decay_iters - warmup_iters)\n",
        "    coeff = 0.5 * (1.0 + np.cos(np.pi * decay_ratio))\n",
        "    return min_lr + coeff * (learning_rate - min_lr)\n",
        "\n",
        "def train_model(model, train_data, val_data, max_iters, learning_rate, out_dir, dataset_name):\n",
        "    model = model.to(device)\n",
        "    model.train()\n",
        "    \n",
        "    optimizer = model.configure_optimizers(weight_decay, learning_rate, (beta1, beta2), device)\n",
        "    scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))\n",
        "    \n",
        "    dtype_torch = torch.bfloat16 if 'bfloat' in dtype else torch.float16\n",
        "    if device == 'cuda':\n",
        "        dtype_ctx = torch.amp.autocast(device_type=device, dtype=dtype_torch)\n",
        "    else:\n",
        "        dtype_ctx = nullcontext()\n",
        "    \n",
        "    history = {'train_loss': [], 'val_loss': [], 'lr': [], 'iter': [], 'time': []}\n",
        "    os.makedirs(out_dir, exist_ok=True)\n",
        "    \n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"Training on {dataset_name}\")\n",
        "    print(f\"{'='*60}\")\n",
        "    print(f\"  Max iterations: {max_iters}\")\n",
        "    print(f\"  Learning rate: {learning_rate}\")\n",
        "    print(f\"  Parameters: {model.get_num_params()/1e6:.2f}M\")\n",
        "    print(f\"  Train tokens: {len(train_data):,}\")\n",
        "    print(f\"{'='*60}\")\n",
        "    \n",
        "    t0 = time.time()\n",
        "    best_val_loss = float('inf')\n",
        "    \n",
        "    for iter_num in range(max_iters):\n",
        "        lr = get_lr(iter_num, warmup_iters, max_iters, learning_rate, min_lr) if decay_lr else learning_rate\n",
        "        for param_group in optimizer.param_groups:\n",
        "            param_group['lr'] = lr\n",
        "        \n",
        "        X, Y = get_batch(train_data, batch_size, block_size, device)\n",
        "        \n",
        "        with dtype_ctx:\n",
        "            _, loss = model(X, Y)\n",
        "        \n",
        "        scaler.scale(loss).backward()\n",
        "        if grad_clip != 0.0:\n",
        "            scaler.unscale_(optimizer)\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "        \n",
        "        if iter_num % eval_interval == 0 or iter_num == max_iters - 1:\n",
        "            t1 = time.time()\n",
        "            losses = estimate_loss(model, train_data, val_data, batch_size, block_size, eval_iters, device, dtype_ctx)\n",
        "            dt = t1 - t0\n",
        "            tokens_per_sec = batch_size * block_size * eval_interval / dt if dt > 0 else 0\n",
        "            \n",
        "            print(f\"  Iter {iter_num:4d}/{max_iters}: train={losses['train']:.4f}, val={losses['val']:.4f}, lr={lr:.6f}, {tokens_per_sec:.0f} tok/s\")\n",
        "            \n",
        "            history['train_loss'].append(losses['train'])\n",
        "            history['val_loss'].append(losses['val'])\n",
        "            history['lr'].append(lr)\n",
        "            history['iter'].append(iter_num)\n",
        "            history['time'].append(time.time() - t0)\n",
        "            \n",
        "            if losses['val'] < best_val_loss:\n",
        "                best_val_loss = losses['val']\n",
        "                checkpoint = {\n",
        "                    'model': model.state_dict(),\n",
        "                    'config': model.config,\n",
        "                    'iter_num': iter_num,\n",
        "                    'best_val_loss': best_val_loss,\n",
        "                }\n",
        "                torch.save(checkpoint, os.path.join(out_dir, 'ckpt.pt'))\n",
        "            \n",
        "            t0 = time.time()\n",
        "    \n",
        "    total_time = sum(history['time'])\n",
        "    print(f\"\\n  Training completed in {total_time/60:.1f} minutes\")\n",
        "    print(f\"  Best validation loss: {best_val_loss:.4f}\")\n",
        "    print(f\"  Best validation perplexity: {np.exp(best_val_loss):.2f}\")\n",
        "    \n",
        "    return history\n",
        "\n",
        "print(\"Training functions defined!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training All 4 Models\n",
        "\n",
        "This cell trains all 4 model configurations sequentially:\n",
        "1. WikiText-2 without RoPE\n",
        "2. WikiText-2 with RoPE\n",
        "3. BookCorpus without RoPE\n",
        "4. BookCorpus with RoPE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "histories = {}\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"STARTING TRAINING OF ALL 4 MODELS\")\n",
        "print(\"=\"*70)\n",
        "print(f\"\\nModels to train:\")\n",
        "for i, (name, (_, _, _, display_name, _)) in enumerate(models.items(), 1):\n",
        "    print(f\"  {i}. {display_name}\")\n",
        "print(f\"\\nTotal iterations per model: {max_iters}\")\n",
        "print(f\"Estimated time: ~3-5 hours on GPU\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "training_start = time.time()\n",
        "\n",
        "for model_name, (model, train_data, val_data, display_name, out_dir) in models.items():\n",
        "    print(f\"\\n{'#'*70}\")\n",
        "    print(f\"# TRAINING MODEL: {display_name.upper()}\")\n",
        "    print(f\"{'#'*70}\")\n",
        "    \n",
        "    history = train_model(\n",
        "        model=model,\n",
        "        train_data=train_data,\n",
        "        val_data=val_data,\n",
        "        max_iters=max_iters,\n",
        "        learning_rate=learning_rate,\n",
        "        out_dir=out_dir,\n",
        "        dataset_name=display_name\n",
        "    )\n",
        "    \n",
        "    histories[model_name] = history\n",
        "    \n",
        "    # Clear CUDA cache between models\n",
        "    if device == 'cuda':\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "total_training_time = time.time() - training_start\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"ALL MODELS TRAINED SUCCESSFULLY!\")\n",
        "print(\"=\"*70)\n",
        "print(f\"Total training time: {total_training_time/3600:.2f} hours\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Final Evaluation and Results Extraction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Extract final results from all models\n",
        "results = {}\n",
        "\n",
        "for model_name, history in histories.items():\n",
        "    results[model_name] = {\n",
        "        'train_loss': history['train_loss'][-1],\n",
        "        'val_loss': history['val_loss'][-1],\n",
        "        'train_ppl': np.exp(history['train_loss'][-1]),\n",
        "        'val_ppl': np.exp(history['val_loss'][-1]),\n",
        "        'best_val_loss': min(history['val_loss']),\n",
        "        'best_val_ppl': np.exp(min(history['val_loss'])),\n",
        "        'total_time': history['time'][-1] if history['time'] else 0,\n",
        "    }\n",
        "\n",
        "# Display results table\n",
        "print(\"=\"*90)\n",
        "print(\"FINAL RESULTS SUMMARY\")\n",
        "print(\"=\"*90)\n",
        "print(f\"{'Model':<30} {'Train Loss':>12} {'Val Loss':>12} {'Train PPL':>12} {'Val PPL':>12}\")\n",
        "print(\"-\"*90)\n",
        "\n",
        "display_names = {\n",
        "    'wikitext_no_rope': 'WikiText-2 (No RoPE)',\n",
        "    'wikitext_rope': 'WikiText-2 (RoPE)',\n",
        "    'bookcorpus_no_rope': 'BookCorpus (No RoPE)',\n",
        "    'bookcorpus_rope': 'BookCorpus (RoPE)',\n",
        "}\n",
        "\n",
        "for model_name, res in results.items():\n",
        "    print(f\"{display_names[model_name]:<30} {res['train_loss']:>12.4f} {res['val_loss']:>12.4f} {res['train_ppl']:>12.2f} {res['val_ppl']:>12.2f}\")\n",
        "\n",
        "print(\"=\"*90)\n",
        "\n",
        "# Calculate improvements\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"ROPE IMPROVEMENT ANALYSIS\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# WikiText-2 improvement\n",
        "wiki_val_improvement = (results['wikitext_no_rope']['val_ppl'] - results['wikitext_rope']['val_ppl']) / results['wikitext_no_rope']['val_ppl'] * 100\n",
        "wiki_train_improvement = (results['wikitext_no_rope']['train_ppl'] - results['wikitext_rope']['train_ppl']) / results['wikitext_no_rope']['train_ppl'] * 100\n",
        "\n",
        "print(f\"\\nWikiText-2:\")\n",
        "print(f\"  Without RoPE - Val PPL: {results['wikitext_no_rope']['val_ppl']:.2f}\")\n",
        "print(f\"  With RoPE    - Val PPL: {results['wikitext_rope']['val_ppl']:.2f}\")\n",
        "print(f\"  Improvement: {wiki_val_improvement:+.2f}% (positive = RoPE better)\")\n",
        "\n",
        "# BookCorpus improvement\n",
        "book_val_improvement = (results['bookcorpus_no_rope']['val_ppl'] - results['bookcorpus_rope']['val_ppl']) / results['bookcorpus_no_rope']['val_ppl'] * 100\n",
        "book_train_improvement = (results['bookcorpus_no_rope']['train_ppl'] - results['bookcorpus_rope']['train_ppl']) / results['bookcorpus_no_rope']['train_ppl'] * 100\n",
        "\n",
        "print(f\"\\nBookCorpus:\")\n",
        "print(f\"  Without RoPE - Val PPL: {results['bookcorpus_no_rope']['val_ppl']:.2f}\")\n",
        "print(f\"  With RoPE    - Val PPL: {results['bookcorpus_rope']['val_ppl']:.2f}\")\n",
        "print(f\"  Improvement: {book_val_improvement:+.2f}% (positive = RoPE better)\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Visualizations\n",
        "\n",
        "### Figure 1: Training Loss Curves"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
        "\n",
        "# Colors and styles\n",
        "colors = {\n",
        "    'wikitext_no_rope': '#1f77b4',\n",
        "    'wikitext_rope': '#ff7f0e',\n",
        "    'bookcorpus_no_rope': '#2ca02c',\n",
        "    'bookcorpus_rope': '#d62728',\n",
        "}\n",
        "\n",
        "linestyles = {\n",
        "    'wikitext_no_rope': '--',\n",
        "    'wikitext_rope': '-',\n",
        "    'bookcorpus_no_rope': '--',\n",
        "    'bookcorpus_rope': '-',\n",
        "}\n",
        "\n",
        "# Plot 1: Training Loss\n",
        "ax1 = axes[0]\n",
        "for model_name, history in histories.items():\n",
        "    ax1.plot(history['iter'], history['train_loss'], \n",
        "             label=display_names[model_name],\n",
        "             color=colors[model_name],\n",
        "             linestyle=linestyles[model_name],\n",
        "             linewidth=2)\n",
        "\n",
        "ax1.set_xlabel('Iteration', fontsize=12)\n",
        "ax1.set_ylabel('Training Loss', fontsize=12)\n",
        "ax1.set_title('Training Loss Over Time', fontsize=14)\n",
        "ax1.legend(loc='upper right')\n",
        "ax1.grid(True, alpha=0.3)\n",
        "\n",
        "# Plot 2: Validation Loss\n",
        "ax2 = axes[1]\n",
        "for model_name, history in histories.items():\n",
        "    ax2.plot(history['iter'], history['val_loss'],\n",
        "             label=display_names[model_name],\n",
        "             color=colors[model_name],\n",
        "             linestyle=linestyles[model_name],\n",
        "             linewidth=2)\n",
        "\n",
        "ax2.set_xlabel('Iteration', fontsize=12)\n",
        "ax2.set_ylabel('Validation Loss', fontsize=12)\n",
        "ax2.set_title('Validation Loss Over Time', fontsize=14)\n",
        "ax2.legend(loc='upper right')\n",
        "ax2.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('rope_comparison_loss.png', dpi=150, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(\"Figure saved: rope_comparison_loss.png\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Figure 2: Perplexity Over Time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
        "\n",
        "# Plot 1: Training Perplexity\n",
        "ax1 = axes[0]\n",
        "for model_name, history in histories.items():\n",
        "    ppl = [np.exp(l) for l in history['train_loss']]\n",
        "    ax1.plot(history['iter'], ppl,\n",
        "             label=display_names[model_name],\n",
        "             color=colors[model_name],\n",
        "             linestyle=linestyles[model_name],\n",
        "             linewidth=2)\n",
        "\n",
        "ax1.set_xlabel('Iteration', fontsize=12)\n",
        "ax1.set_ylabel('Training Perplexity', fontsize=12)\n",
        "ax1.set_title('Training Perplexity Over Time', fontsize=14)\n",
        "ax1.legend(loc='upper right')\n",
        "ax1.grid(True, alpha=0.3)\n",
        "ax1.set_yscale('log')\n",
        "\n",
        "# Plot 2: Validation Perplexity\n",
        "ax2 = axes[1]\n",
        "for model_name, history in histories.items():\n",
        "    ppl = [np.exp(l) for l in history['val_loss']]\n",
        "    ax2.plot(history['iter'], ppl,\n",
        "             label=display_names[model_name],\n",
        "             color=colors[model_name],\n",
        "             linestyle=linestyles[model_name],\n",
        "             linewidth=2)\n",
        "\n",
        "ax2.set_xlabel('Iteration', fontsize=12)\n",
        "ax2.set_ylabel('Validation Perplexity', fontsize=12)\n",
        "ax2.set_title('Validation Perplexity Over Time', fontsize=14)\n",
        "ax2.legend(loc='upper right')\n",
        "ax2.grid(True, alpha=0.3)\n",
        "ax2.set_yscale('log')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('rope_comparison_perplexity.png', dpi=150, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(\"Figure saved: rope_comparison_perplexity.png\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Figure 3: Final Results Comparison (Bar Charts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
        "\n",
        "# Bar positions\n",
        "x = np.arange(2)  # WikiText-2, BookCorpus\n",
        "width = 0.35\n",
        "\n",
        "# Data for plotting\n",
        "datasets = ['WikiText-2', 'BookCorpus']\n",
        "no_rope_val_ppl = [results['wikitext_no_rope']['val_ppl'], results['bookcorpus_no_rope']['val_ppl']]\n",
        "rope_val_ppl = [results['wikitext_rope']['val_ppl'], results['bookcorpus_rope']['val_ppl']]\n",
        "no_rope_val_loss = [results['wikitext_no_rope']['val_loss'], results['bookcorpus_no_rope']['val_loss']]\n",
        "rope_val_loss = [results['wikitext_rope']['val_loss'], results['bookcorpus_rope']['val_loss']]\n",
        "\n",
        "# Plot 1: Validation Loss Comparison\n",
        "ax1 = axes[0]\n",
        "bars1 = ax1.bar(x - width/2, no_rope_val_loss, width, label='No RoPE', color='#1f77b4', alpha=0.8)\n",
        "bars2 = ax1.bar(x + width/2, rope_val_loss, width, label='RoPE', color='#ff7f0e', alpha=0.8)\n",
        "\n",
        "ax1.set_xlabel('Dataset', fontsize=12)\n",
        "ax1.set_ylabel('Validation Loss', fontsize=12)\n",
        "ax1.set_title('Final Validation Loss Comparison', fontsize=14)\n",
        "ax1.set_xticks(x)\n",
        "ax1.set_xticklabels(datasets)\n",
        "ax1.legend()\n",
        "ax1.grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "# Add value labels on bars\n",
        "for bar in bars1:\n",
        "    height = bar.get_height()\n",
        "    ax1.annotate(f'{height:.3f}', xy=(bar.get_x() + bar.get_width()/2, height),\n",
        "                 xytext=(0, 3), textcoords=\"offset points\", ha='center', va='bottom', fontsize=10)\n",
        "for bar in bars2:\n",
        "    height = bar.get_height()\n",
        "    ax1.annotate(f'{height:.3f}', xy=(bar.get_x() + bar.get_width()/2, height),\n",
        "                 xytext=(0, 3), textcoords=\"offset points\", ha='center', va='bottom', fontsize=10)\n",
        "\n",
        "# Plot 2: Validation Perplexity Comparison\n",
        "ax2 = axes[1]\n",
        "bars3 = ax2.bar(x - width/2, no_rope_val_ppl, width, label='No RoPE', color='#1f77b4', alpha=0.8)\n",
        "bars4 = ax2.bar(x + width/2, rope_val_ppl, width, label='RoPE', color='#ff7f0e', alpha=0.8)\n",
        "\n",
        "ax2.set_xlabel('Dataset', fontsize=12)\n",
        "ax2.set_ylabel('Validation Perplexity', fontsize=12)\n",
        "ax2.set_title('Final Validation Perplexity Comparison', fontsize=14)\n",
        "ax2.set_xticks(x)\n",
        "ax2.set_xticklabels(datasets)\n",
        "ax2.legend()\n",
        "ax2.grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "# Add value labels on bars\n",
        "for bar in bars3:\n",
        "    height = bar.get_height()\n",
        "    ax2.annotate(f'{height:.1f}', xy=(bar.get_x() + bar.get_width()/2, height),\n",
        "                 xytext=(0, 3), textcoords=\"offset points\", ha='center', va='bottom', fontsize=10)\n",
        "for bar in bars4:\n",
        "    height = bar.get_height()\n",
        "    ax2.annotate(f'{height:.1f}', xy=(bar.get_x() + bar.get_width()/2, height),\n",
        "                 xytext=(0, 3), textcoords=\"offset points\", ha='center', va='bottom', fontsize=10)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('rope_comparison_final.png', dpi=150, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(\"Figure saved: rope_comparison_final.png\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Figure 4: Convergence Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def find_convergence_iter(history, threshold_pct):\n",
        "    \"\"\"Find the iteration at which the model reaches threshold_pct of its final loss.\"\"\"\n",
        "    final_loss = history['val_loss'][-1]\n",
        "    initial_loss = history['val_loss'][0]\n",
        "    target_loss = initial_loss - (initial_loss - final_loss) * threshold_pct / 100\n",
        "    \n",
        "    for i, (iter_num, loss) in enumerate(zip(history['iter'], history['val_loss'])):\n",
        "        if loss <= target_loss:\n",
        "            return iter_num\n",
        "    return history['iter'][-1]\n",
        "\n",
        "# Calculate convergence iterations\n",
        "convergence_data = {}\n",
        "thresholds = [50, 75, 90, 95]\n",
        "\n",
        "for model_name, history in histories.items():\n",
        "    convergence_data[model_name] = {}\n",
        "    for threshold in thresholds:\n",
        "        convergence_data[model_name][threshold] = find_convergence_iter(history, threshold)\n",
        "\n",
        "# Plot convergence comparison\n",
        "fig, ax = plt.subplots(figsize=(12, 6))\n",
        "\n",
        "x = np.arange(len(thresholds))\n",
        "width = 0.2\n",
        "\n",
        "model_names = list(histories.keys())\n",
        "for i, model_name in enumerate(model_names):\n",
        "    values = [convergence_data[model_name][t] for t in thresholds]\n",
        "    ax.bar(x + i*width - 1.5*width, values, width, \n",
        "           label=display_names[model_name],\n",
        "           color=colors[model_name],\n",
        "           alpha=0.8)\n",
        "\n",
        "ax.set_xlabel('Convergence Threshold (%)', fontsize=12)\n",
        "ax.set_ylabel('Iterations to Reach Threshold', fontsize=12)\n",
        "ax.set_title('Convergence Speed Comparison', fontsize=14)\n",
        "ax.set_xticks(x)\n",
        "ax.set_xticklabels([f'{t}%' for t in thresholds])\n",
        "ax.legend(loc='upper left')\n",
        "ax.grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('rope_comparison_convergence.png', dpi=150, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(\"Figure saved: rope_comparison_convergence.png\")\n",
        "\n",
        "# Print convergence table\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"CONVERGENCE SPEED (iterations to reach X% of final improvement)\")\n",
        "print(\"=\"*80)\n",
        "print(f\"{'Model':<30} {'50%':>10} {'75%':>10} {'90%':>10} {'95%':>10}\")\n",
        "print(\"-\"*80)\n",
        "for model_name in model_names:\n",
        "    values = [convergence_data[model_name][t] for t in thresholds]\n",
        "    print(f\"{display_names[model_name]:<30} {values[0]:>10} {values[1]:>10} {values[2]:>10} {values[3]:>10}\")\n",
        "print(\"=\"*80)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Text Generation Comparison\n",
        "\n",
        "Generate text samples from all 4 models to qualitatively compare their outputs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "enc = tiktoken.get_encoding(\"gpt2\")\n",
        "\n",
        "@torch.no_grad()\n",
        "def generate_text(model, prompt, max_new_tokens=100, temperature=0.8, top_k=40):\n",
        "    model.eval()\n",
        "    model = model.to(device)\n",
        "    \n",
        "    tokens = enc.encode(prompt)\n",
        "    x = torch.tensor(tokens, dtype=torch.long, device=device).unsqueeze(0)\n",
        "    \n",
        "    dtype_torch = torch.bfloat16 if 'bfloat' in dtype else torch.float16\n",
        "    if device == 'cuda':\n",
        "        dtype_ctx = torch.amp.autocast(device_type=device, dtype=dtype_torch)\n",
        "    else:\n",
        "        dtype_ctx = nullcontext()\n",
        "    \n",
        "    with dtype_ctx:\n",
        "        generated = model.generate(x, max_new_tokens=max_new_tokens, temperature=temperature, top_k=top_k)\n",
        "    \n",
        "    return enc.decode(generated[0].tolist())\n",
        "\n",
        "# Test prompts\n",
        "prompts = [\n",
        "    \"The history of artificial intelligence\",\n",
        "    \"Once upon a time, in a distant land,\",\n",
        "    \"The most important scientific discovery\",\n",
        "]\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"TEXT GENERATION COMPARISON\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "for prompt in prompts:\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(f\"PROMPT: \\\"{prompt}\\\"\")\n",
        "    print(\"=\"*80)\n",
        "    \n",
        "    for model_name, (model, _, _, display_name, _) in models.items():\n",
        "        print(f\"\\n--- {display_name} ---\")\n",
        "        try:\n",
        "            generated = generate_text(model, prompt, max_new_tokens=80)\n",
        "            print(generated)\n",
        "        except Exception as e:\n",
        "            print(f\"Error generating text: {e}\")\n",
        "    \n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Overfitting Analysis\n",
        "\n",
        "Compare the train-validation gap to assess if RoPE affects overfitting."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calculate train-val gap (overfitting indicator)\n",
        "print(\"=\"*80)\n",
        "print(\"OVERFITTING ANALYSIS (Train-Val Gap)\")\n",
        "print(\"=\"*80)\n",
        "print(f\"{'Model':<30} {'Train Loss':>12} {'Val Loss':>12} {'Gap':>12} {'Gap %':>12}\")\n",
        "print(\"-\"*80)\n",
        "\n",
        "gaps = {}\n",
        "for model_name, res in results.items():\n",
        "    gap = res['val_loss'] - res['train_loss']\n",
        "    gap_pct = (res['val_loss'] - res['train_loss']) / res['train_loss'] * 100\n",
        "    gaps[model_name] = {'gap': gap, 'gap_pct': gap_pct}\n",
        "    print(f\"{display_names[model_name]:<30} {res['train_loss']:>12.4f} {res['val_loss']:>12.4f} {gap:>12.4f} {gap_pct:>11.2f}%\")\n",
        "\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Compare RoPE effect on overfitting\n",
        "print(\"\\nRoPE Effect on Overfitting:\")\n",
        "wiki_gap_change = gaps['wikitext_rope']['gap_pct'] - gaps['wikitext_no_rope']['gap_pct']\n",
        "book_gap_change = gaps['bookcorpus_rope']['gap_pct'] - gaps['bookcorpus_no_rope']['gap_pct']\n",
        "\n",
        "print(f\"  WikiText-2: Gap change with RoPE: {wiki_gap_change:+.2f}% (negative = less overfitting)\")\n",
        "print(f\"  BookCorpus: Gap change with RoPE: {book_gap_change:+.2f}% (negative = less overfitting)\")\n",
        "\n",
        "# Plot overfitting comparison\n",
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "\n",
        "x = np.arange(len(results))\n",
        "model_labels = [display_names[m] for m in results.keys()]\n",
        "train_losses = [res['train_loss'] for res in results.values()]\n",
        "val_losses = [res['val_loss'] for res in results.values()]\n",
        "\n",
        "width = 0.35\n",
        "ax.bar(x - width/2, train_losses, width, label='Train Loss', color='#2ecc71', alpha=0.8)\n",
        "ax.bar(x + width/2, val_losses, width, label='Val Loss', color='#e74c3c', alpha=0.8)\n",
        "\n",
        "ax.set_xlabel('Model', fontsize=12)\n",
        "ax.set_ylabel('Loss', fontsize=12)\n",
        "ax.set_title('Train vs Validation Loss (Overfitting Analysis)', fontsize=14)\n",
        "ax.set_xticks(x)\n",
        "ax.set_xticklabels(model_labels, rotation=15, ha='right')\n",
        "ax.legend()\n",
        "ax.grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('rope_comparison_overfitting.png', dpi=150, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nFigure saved: rope_comparison_overfitting.png\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary and Interpretation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\"*80)\n",
        "print(\"FINAL SUMMARY: RoPE vs STANDARD POSITION EMBEDDINGS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# WikiText-2 Analysis\n",
        "print(\"\\n\" + \"-\"*40)\n",
        "print(\"WIKITEXT-2 (Encyclopedic Text)\")\n",
        "print(\"-\"*40)\n",
        "print(f\"  Without RoPE:\")\n",
        "print(f\"    - Validation Loss: {results['wikitext_no_rope']['val_loss']:.4f}\")\n",
        "print(f\"    - Validation PPL:  {results['wikitext_no_rope']['val_ppl']:.2f}\")\n",
        "print(f\"  With RoPE:\")\n",
        "print(f\"    - Validation Loss: {results['wikitext_rope']['val_loss']:.4f}\")\n",
        "print(f\"    - Validation PPL:  {results['wikitext_rope']['val_ppl']:.2f}\")\n",
        "print(f\"  Improvement: {wiki_val_improvement:+.2f}%\")\n",
        "\n",
        "if wiki_val_improvement > 0:\n",
        "    print(f\"  - RoPE IMPROVES performance on WikiText-2\")\n",
        "elif wiki_val_improvement < 0:\n",
        "    print(f\"  - RoPE DEGRADES performance on WikiText-2\")\n",
        "else:\n",
        "    print(f\"  - No significant difference on WikiText-2\")\n",
        "\n",
        "# BookCorpus Analysis\n",
        "print(\"\\n\" + \"-\"*40)\n",
        "print(\"BOOKCORPUS (Narrative Text)\")\n",
        "print(\"-\"*40)\n",
        "print(f\"  Without RoPE:\")\n",
        "print(f\"    - Validation Loss: {results['bookcorpus_no_rope']['val_loss']:.4f}\")\n",
        "print(f\"    - Validation PPL:  {results['bookcorpus_no_rope']['val_ppl']:.2f}\")\n",
        "print(f\"  With RoPE:\")\n",
        "print(f\"    - Validation Loss: {results['bookcorpus_rope']['val_loss']:.4f}\")\n",
        "print(f\"    - Validation PPL:  {results['bookcorpus_rope']['val_ppl']:.2f}\")\n",
        "print(f\"  Improvement: {book_val_improvement:+.2f}%\")\n",
        "\n",
        "if book_val_improvement > 0:\n",
        "    print(f\"  - RoPE IMPROVES performance on BookCorpus\")\n",
        "elif book_val_improvement < 0:\n",
        "    print(f\"  - RoPE DEGRADES performance on BookCorpus\")\n",
        "else:\n",
        "    print(f\"  - No significant difference on BookCorpus\")\n",
        "\n",
        "# Overall Analysis\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"OVERALL ANALYSIS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "avg_improvement = (wiki_val_improvement + book_val_improvement) / 2\n",
        "print(f\"\\nAverage RoPE improvement across datasets: {avg_improvement:+.2f}%\")\n",
        "\n",
        "# Effect consistency\n",
        "effect_diff = abs(wiki_val_improvement - book_val_improvement)\n",
        "print(f\"\\nEffect consistency:\")\n",
        "if effect_diff < 2:\n",
        "    print(f\"  - RoPE has CONSISTENT effect across text styles (diff: {effect_diff:.2f}%)\")\n",
        "elif effect_diff < 5:\n",
        "    print(f\"  - RoPE has MODERATELY DIFFERENT effect across text styles (diff: {effect_diff:.2f}%)\")\n",
        "else:\n",
        "    print(f\"  - RoPE has SIGNIFICANTLY DIFFERENT effect across text styles (diff: {effect_diff:.2f}%)\")\n",
        "\n",
        "# Conclusion\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"CONCLUSIONS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "if avg_improvement > 2:\n",
        "    print(\"\\n1. RoPE provides a measurable improvement over standard position embeddings.\")\n",
        "elif avg_improvement < -2:\n",
        "    print(\"\\n1. Standard position embeddings outperform RoPE in this configuration.\")\n",
        "else:\n",
        "    print(\"\\n1. RoPE and standard position embeddings perform similarly.\")\n",
        "\n",
        "if wiki_val_improvement > book_val_improvement:\n",
        "    print(\"2. RoPE benefits encyclopedic text (WikiText-2) more than narrative text.\")\n",
        "elif book_val_improvement > wiki_val_improvement:\n",
        "    print(\"2. RoPE benefits narrative text (BookCorpus) more than encyclopedic text.\")\n",
        "else:\n",
        "    print(\"2. RoPE provides similar benefits across both text styles.\")\n",
        "\n",
        "# Overfitting conclusion\n",
        "avg_gap_change = (wiki_gap_change + book_gap_change) / 2\n",
        "if avg_gap_change < -1:\n",
        "    print(\"3. RoPE helps reduce overfitting (smaller train-val gap).\")\n",
        "elif avg_gap_change > 1:\n",
        "    print(\"3. RoPE slightly increases overfitting tendency.\")\n",
        "else:\n",
        "    print(\"3. RoPE has minimal effect on overfitting.\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"EXPERIMENT COMPLETED SUCCESSFULLY\")\n",
        "print(\"=\"*80)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Save Results Report"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save comprehensive results to file\n",
        "report = []\n",
        "report.append(\"=\"*80)\n",
        "report.append(\"RoPE vs Standard Position Embeddings - Comparison Report\")\n",
        "report.append(\"=\"*80)\n",
        "report.append(f\"\\nExperiment Date: {time.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
        "report.append(f\"Total Training Time: {total_training_time/3600:.2f} hours\")\n",
        "report.append(\"\\n\" + \"=\"*80)\n",
        "report.append(\"MODEL CONFIGURATION\")\n",
        "report.append(\"=\"*80)\n",
        "report.append(f\"  Layers: {n_layer}\")\n",
        "report.append(f\"  Heads: {n_head}\")\n",
        "report.append(f\"  Embedding dim: {n_embd}\")\n",
        "report.append(f\"  Block size: {block_size}\")\n",
        "report.append(f\"  Parameters: {num_params/1e6:.2f}M\")\n",
        "report.append(f\"  RoPE theta: {rope_theta}\")\n",
        "report.append(\"\\n\" + \"=\"*80)\n",
        "report.append(\"TRAINING CONFIGURATION\")\n",
        "report.append(\"=\"*80)\n",
        "report.append(f\"  Max iterations: {max_iters}\")\n",
        "report.append(f\"  Batch size: {batch_size}\")\n",
        "report.append(f\"  Learning rate: {learning_rate}\")\n",
        "report.append(f\"  Weight decay: {weight_decay}\")\n",
        "report.append(f\"  Dropout: {dropout}\")\n",
        "report.append(\"\\n\" + \"=\"*80)\n",
        "report.append(\"FINAL RESULTS\")\n",
        "report.append(\"=\"*80)\n",
        "report.append(f\"{'Model':<30} {'Val Loss':>12} {'Val PPL':>12}\")\n",
        "report.append(\"-\"*60)\n",
        "for model_name, res in results.items():\n",
        "    report.append(f\"{display_names[model_name]:<30} {res['val_loss']:>12.4f} {res['val_ppl']:>12.2f}\")\n",
        "report.append(\"\\n\" + \"=\"*80)\n",
        "report.append(\"ROPE IMPROVEMENT\")\n",
        "report.append(\"=\"*80)\n",
        "report.append(f\"  WikiText-2: {wiki_val_improvement:+.2f}%\")\n",
        "report.append(f\"  BookCorpus: {book_val_improvement:+.2f}%\")\n",
        "report.append(f\"  Average: {avg_improvement:+.2f}%\")\n",
        "report.append(\"\\n\" + \"=\"*80)\n",
        "\n",
        "# Write report\n",
        "report_text = \"\\n\".join(report)\n",
        "with open('rope_comparison_summary.txt', 'w') as f:\n",
        "    f.write(report_text)\n",
        "\n",
        "print(report_text)\n",
        "print(\"\\nReport saved to: rope_comparison_summary.txt\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
