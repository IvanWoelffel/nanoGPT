## BookCorpus dataset

after running `prepare.py` (preprocess) we get:

- train.bin is ~X.XX GB, val.bin ~X.XX MB
- train has ~X.XX B tokens
- val has ~X.XX M tokens

this came from 74,003,728 documents in total.

references:

- BookCorpus paper: Zhu et al. (2015) "Aligning Books and Movies"
- Original dataset: https://yknzhu.wixsite.com/mbweb
- HuggingFace: https://huggingface.co/datasets/SamuelYang/bookcorpus
